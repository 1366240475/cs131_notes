\documentclass{article}

\usepackage[final]{style}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{verbatim}
\usepackage{graphicx}       % for figures
\usepackage[]{algorithm2e}
\usepackage{amsmath}


\title{Lecture \#19: Introduction to Deep Learning}

\author{
  Robert Yang \\
  Department of Computer Science\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{\{bobyang9\}@cs.stanford.edu} \\
}

\begin{document}

\maketitle

% Done up to Section 2

\section{History of Deep Learning}
Deep Learning is an important technique that can be used for numerous applications including computer vision. How were deep learning methods developed? Historically, the perceptron, the first algorithm that led to deep learning models, was published in a paper by Rosenblatt in 1957. As we will see later, multiple perceptrons could be used together to form a linear classifier. In the 1980s, further improvements on the algorithm such as backpropagation were made and researchers started training the linear classifiers, albeit not very well. In the 1990s, researchers focused on graphical models, and in the 2000s, the support vector machine was very popular. However, in the 2010s, we have revisited the 1980s and the reliance on backpropagation to automatically learn our models. Due to vastly improved computing power (e.g. GPUs) and a large amount of data, the formerly clunky algorithms in the 1980s have become useful and accurate. But to understand today’s advances in deep learning, we must first understand its foundation based in history: the perceptron model, linear classifier, and backpropagation.

\section{The Perceptron}
The structure of the perceptron model is defined by a few key aspects. First, the perceptron takes in some input which we can call $x$. $x$ is a vector, because there can be multiple inputs (e.g. $x_1$, $x_2$, …, $x_n$). Second, the perceptron returns one output which we can call $y$. In the original version of the perceptron, $y$ would be a scalar (one output only). Between the input and the output, the perceptron contains a weighting function that combines the inputs of the perceptron ($x_1$, $x_2$, …, $x_n$) with weights $w$. $w$ is a vector, because there is one weight for each input. Lastly, between the weighting function and the output, there is a sign function, which is usually the following:

\[   \left\{
\begin{array}{ll}
      1 & xw \geq 0 \\
      0 & otherwise \\
\end{array} 
\right. \]

The perceptron algorithm was inspired from biology, but it is not how the brain works! The brain has neurons which gets input through its dendrites, processes that input in the cell body, and then outputs using the axon which might be sent to other neurons. Similarly, the perceptron takes inputs, processes that input, and then outputs the processed result. The weights of the perceptron are analogous to the strength of the connection across the synapses between neurons. However, neurons are probably much more complex than a simple weighting function, which is why the perceptron algorithm is only loosely inspired.

We can gain intuition of the perceptron algorithm by looking at an application related to computer vision. For example, the inputs to the perceptron might represent some sort of image (and we can do this in many different ways, such as using raw pixels, PCA, and BoW, among others). We can convert this input into an output that might signify whether the input is some type of class (e.g. airplane, bird, etc.)

\section{Linear Classifier}

A linear classifier consists of multiple perceptrons put together. For example, you could use a linear classifier to make predictions for multiple classes.

The structure of a linear classifier is similar to the structure of the perceptron. Continuing our computer vision example, we see that the input is the same – a vector $x$ that may perhaps contain information about an image, whether that be a raw pixel representation or some other kind of feature representation. However, the output is now a set of scores (not just one score). For example, you could have a score for the airplane class, for the bird class, etc. We could classify by using the class that has the highest score.

The conversion from input to output in a linear classifier is also defined by the weights $w$, similar to those in a perceptron. $w$ is a 2-D matrix with dimensions (number of classes, number of input elements). We will show why in the example below.

\bigskip

\textbf{Example 1.}

For a certain linear classifier, let there be 10 classes, and let the input be a 32 x 32 x 3 image. Assume that we are using a raw pixel representation. What is the shape of the weight matrix for the classifier?

Answer: First, we must find how many inputs there are. Because we are using a raw pixel representation, the number of inputs will be the number of pixels, which is 32 * 32 * 3 = 3072 inputs. We also know that there are 10 outputs because there are 10 classes (1 output for each class). For each class, we have a set of 3072 weights, and we have 10 sets in total. This will be organized in a (3072, 10) shaped matrix.

\bigskip

As in the example, each class has a set of $n_{input}$ weights, where $n_{input}$ is the dimension of the input vector. You can actually visualize the weights as images:

\includegraphics[width=400px]{cs131_pic_1.png}

Sometimes, you also add a bias vector to your results. Intuitively, this helps learn the bias for or against one class in the dataset. For example, if there were more birds in the dataset, you would have a big bias term for the element corresponding to the bird class.

\section{Loss Function}

We need to pick the weights so that they minimize the error our model makes on the dataset. Formally, this is $min_W$ Loss($y$, $\hat{y}$), where $y$ is the true label and $\hat{y}$ is the predicted label.

% The following 2 paragraphs adapted from lecture slide 40.
Given several training examples ${(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)}$ and a perceptron $\hat{y} = wx$, where $x_i$ is an image and $y_i$ is an integer label (0 for dog, 1 for cat, etc.)

A loss function $L_i(y_i, \hat{y_i})$ tells us how good our current classifier is. When the classifier predicts correctly ($y_i$ is close to $\hat{y}$), the loss should be low. When the classifier predicts incorrectly ($y_i$ is far from $\hat{y}$), the loss should be high. We can average the loss over the entire dataset to see how well our model is doing in general.

\subsection{Different Types of Loss Functions}

\textbf{L2} is the squared error loss function. The goal is to minimize the squared difference between the ground truth labels and the predictions. However, the L2 loss is not very robust to outliers. One relatively strange example will increase the loss by a lot.

Formula for L2 loss: $L_i(y_i, \hat{y_i}) = (y_i - \hat{y_i})^2$

\textbf{L1} is a loss function similar to L2 which aims to minimize the absolute value of the difference between the ground truth labels and the predictions. The absolute value is there because you want loss to be positive, and zero loss means that you are able to perfectly classify your dataset.

Formula for L1 loss: $L_i(y_i, \hat{y_i}) = |y_i - \hat{y_i}|$

\textbf{Zero-One} is a loss function that only measures if the predictions are correct or incorrect, and does not measure how correct or incorrect the predictions are. If the model makes an error, then the loss is 1, and if the model does not make an error, then the loss is 0. This loss function has a few weaknesses in that models using this loss function takes longer to train (it is less informative on the correctness of the predictions).

Formula for Zero-One loss: $L_i(y_i, \hat{y_i}) = 1||y_i \neq \hat{y_i}||$

\textbf{Hinge} is another loss function that grows bigger when the difference between the prediction and the ground truth increases.

Formula for Hinge loss: $L_i(y_i, \hat{y_i}) = max(0, 1-y_i\hat{y_i})$

\section{The KL Divergence Loss Function and the Softmax Linear Classifier}

Sometimes we want the model to output probabilities instead of scores because we can visualize probabilities better. We need to convert the vector of scores into a probability distribution.

There are no limits to the output space for the scores of the linear classifier model, which mean that values might appear which do not signify probabilities (values greater than 1 or less than 0). Softmax is a method to convert the output into probability ranges [0, 1]. It does so by taking the exponential of the outputs and then normalizing them, as you will see below in the formula. 

$Prob[f(x_i, W) == k] = \frac{e^{\hat{y}_k}}{\sum_{j}e^{\hat{y}_j}}$

The KL divergence allows you to calculate the distance between two probability distributions. First, we can define $P(y)$ as the ground truth distribution, and $Q(y)$ as the model’s output score distribution (the predictions). KL divergence is defined as the following:

$D_{KL} = \sum_{y}^{} P(y)log\frac{P(y)}{Q(y)}$

Usually, $P(y)$ is probability 1 for one class and 0 for all other classes. In such cases, since $P(y)$ is 0 for all the other classes, [LaTeX from slide 46] can be rewritten as $log (\frac{1}{Q(y)})$, which can be rewritten as $-log(Q(y))$, where $y$ is the class with probability 1 in $P(y)$. 

Notice that softmax and KL divergence work together very well because the exponent and ln will cancel out. Furthermore, inputting the probabilities given by softmax into KL divergence gives a loss from 0 to infinity, which is a good range.

\bigskip

$\textbf{Example 2.}$

Let $C$ be the number of classes in the classifier. In the scenario where the classifier outputs the same result for all the classes, and that softmax is used to normalize output, what would be the KL Divergence loss?

Answer: If the classifier outputs the same result for all the classes, then each class will have a probability of $\frac{1}{C}$ after softmax. We know that the KL divergence loss would be $-log(Q(y))$, and we know that Q(y) is $\frac{1}{C}$ because no matter which of the classes has a 1 in the ground truth, the prediction would be $\frac{1}{C}$. Thus, we would find that the KL divergence loss would be $-log(\frac{1}{C})$, which is $log(C)$.

\bigskip

A summary:

\textbf{Softmax} allows us to convert scores into probabilities.

\textbf{KL Divergence} allows us to calculate the distance between the predicted probabilities and the ground truth.


\section{Gradient Descent}

Gradient Descent is a technique used to find optimal weights that minimize the loss. 

\subsection{Intuition of Gradient Descent}

In Gradient Descent, you iterate through the following loop:

1. You first compute the derivative of the loss with respect to the weights, and with this, you would know which direction to move the weights in order to lower the loss.
2. You would move toward that direction by updating the weights.

You would repeat the two steps until you get to a local minima. In this way, the gradient descent algorithm will slowly decrease the loss.

\subsection{Theory}

We can calculate what the gradient looks like given our loss function. First, we have a training data point (x, y), and we are using the linear classifier where $\hat{y} = wx$. Let us define k as the class which is correct. We know that the loss is -log(softmax($\hat{y}(k)$)), where $\hat{y}(k)$ is the score given to the correct class by the linear classifier.

Evaluating that expression, we get that Loss = $L(\hat{y}, y)$ = $-log\frac{e^{\hat{y}_k}}{\sum_{j}e^{\hat{y}_j}}$ = $-\hat{y}_k + log\sum_j e^{\hat{y}_j}$. Now that we have our loss function in terms of $\hat{y}$, which can be expressed in terms of $w$, we can therefore calculate the derivative of the loss function with respect to each weight.

\subsection{Pseudocode of Gradient Descent}

Now, we can translate the theory into code. First, we calculate the loss by looping through each data point and averaging the losses of each data point. Then, we calculate the derivative of the loss with respect to each weight, and update each weight.

\includegraphics[width=250px]{cs131_pic_2.png}

$\alpha$ is a hyperparameter that represents step size. You can tune this hyperparameter to find the optimal step size. If $\alpha$ is too small, then your algorithm will take a long time (many iterations) to finish, while if $\alpha$ is too big, your algorithm might overshoot the local minima.

% To come next time? 
% \section{Gradient Descent and Backpropagation}
% \section{Neural Networks}
 
\section{Conclusion}
Understanding the history of deep learning and how the algorithms and models developed will be crucial to understanding how deep learning works. The models of deep learning are made of stacks of linked linear classifiers. Deep learning algorithms evaluate their performance using loss functions. Softmax is still popular in the output of deep learning algorithms. Furthermore, backpropagation and gradient descent (advanced versions of it), which give the model the ability to learn weights, is crucial for performance at increasingly complex tasks in computer vision and elsewhere.


%Deep learning heavily uses all the basic tools such as Linear Classifiers, Loss Functions, Softmax, and Gradient Descent. 

%Deep Learning is a data-intensive approach unlike most of the traditional computer vision algorithms. Thus, it is 
% References
\small
\bibliographystyle{plain}
\bibliography{bibliography}

\bigskip
\end{document}